<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Luna Agent Voice Pipeline Test</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            margin: 0;
            padding: 20px;
            background: linear-gradient(135deg, #1a1a1a, #2d1b69);
            color: white;
            min-height: 100vh;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
        }
        
        .test-section {
            background: rgba(255, 255, 255, 0.1);
            border-radius: 12px;
            padding: 20px;
            margin: 20px 0;
            backdrop-filter: blur(10px);
        }
        
        .test-result {
            padding: 10px;
            border-radius: 8px;
            margin: 10px 0;
        }
        
        .pass { background-color: rgba(0, 255, 0, 0.2); border-left: 4px solid #00ff00; }
        .fail { background-color: rgba(255, 0, 0, 0.2); border-left: 4px solid #ff0000; }
        .warn { background-color: rgba(255, 255, 0, 0.2); border-left: 4px solid #ffff00; }
        .info { background-color: rgba(0, 255, 255, 0.2); border-left: 4px solid #00ffff; }
        
        button {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            border: none;
            color: white;
            padding: 12px 24px;
            border-radius: 8px;
            cursor: pointer;
            margin: 5px;
            font-size: 14px;
        }
        
        button:hover {
            transform: translateY(-1px);
            box-shadow: 0 4px 12px rgba(102, 126, 234, 0.4);
        }
        
        button:disabled {
            opacity: 0.5;
            cursor: not-allowed;
        }
        
        .audio-visualizer {
            width: 100%;
            height: 100px;
            background: rgba(0, 0, 0, 0.3);
            border-radius: 8px;
            margin: 10px 0;
        }
        
        .status-indicator {
            display: inline-block;
            width: 12px;
            height: 12px;
            border-radius: 50%;
            margin-right: 8px;
        }
        
        .status-online { background-color: #00ff00; }
        .status-offline { background-color: #ff0000; }
        .status-warning { background-color: #ffff00; }
        
        .log-output {
            background: rgba(0, 0, 0, 0.5);
            color: #00ff00;
            font-family: 'Courier New', monospace;
            padding: 15px;
            border-radius: 8px;
            max-height: 300px;
            overflow-y: auto;
            margin-top: 15px;
        }
        
        .test-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
            margin-top: 20px;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>üåô Luna Agent Voice Pipeline Test Suite</h1>
        <p>Comprehensive testing of voice functionality, TTS, wake word detection, and IPC communication.</p>

        <!-- System Status -->
        <div class="test-section">
            <h2>System Status</h2>
            <div id="system-status">
                <div id="browser-support"></div>
                <div id="api-availability"></div>
                <div id="permissions-status"></div>
            </div>
        </div>

        <!-- Test Controls -->
        <div class="test-section">
            <h2>Voice Pipeline Tests</h2>
            <div class="test-grid">
                <div>
                    <h3>Audio Context Test</h3>
                    <button onclick="testAudioContext()">Test Web Audio API</button>
                    <div id="audio-context-result"></div>
                </div>
                
                <div>
                    <h3>Speech Recognition Test</h3>
                    <button onclick="testSpeechRecognition()">Test Speech Recognition</button>
                    <button onclick="stopSpeechRecognition()">Stop Recognition</button>
                    <div id="speech-recognition-result"></div>
                </div>
                
                <div>
                    <h3>TTS Simulation Test</h3>
                    <button onclick="testTTSSimulation()">Test TTS Audio Playback</button>
                    <button onclick="stopTTSPlayback()">Stop TTS</button>
                    <div id="tts-result"></div>
                </div>
                
                <div>
                    <h3>ElevenLabs API Test</h3>
                    <button onclick="testElevenLabsAPI()">Test ElevenLabs Connection</button>
                    <div id="elevenlabs-result"></div>
                </div>
                
                <div>
                    <h3>Wake Word Test</h3>
                    <button onclick="testPicovoiceAPI()">Test Picovoice Integration</button>
                    <div id="wake-word-result"></div>
                </div>
                
                <div>
                    <h3>Full Voice Pipeline Test</h3>
                    <button onclick="testFullPipeline()">Run Complete Test</button>
                    <div id="full-pipeline-result"></div>
                </div>
            </div>
        </div>

        <!-- Audio Visualizer -->
        <div class="test-section">
            <h2>Audio Visualizer</h2>
            <canvas id="audioCanvas" class="audio-visualizer" width="800" height="100"></canvas>
            <div>
                <button onclick="startAudioVisualizer()">Start Visualizer</button>
                <button onclick="stopAudioVisualizer()">Stop Visualizer</button>
            </div>
        </div>

        <!-- Test Logs -->
        <div class="test-section">
            <h2>Test Logs</h2>
            <div id="log-output" class="log-output"></div>
            <button onclick="clearLogs()">Clear Logs</button>
        </div>
    </div>

    <script>
        // Global variables
        let audioContext = null;
        let speechRecognition = null;
        let currentAudioSource = null;
        let animationFrame = null;
        let mediaStream = null;
        let analyser = null;

        // Logging utility
        function log(message, type = 'info') {
            const timestamp = new Date().toLocaleTimeString();
            const logOutput = document.getElementById('log-output');
            const colorMap = {
                info: '#00ff00',
                error: '#ff0000',
                warn: '#ffff00',
                success: '#00ff88'
            };
            const color = colorMap[type] || '#00ff00';
            logOutput.innerHTML += `<span style="color: ${color}">[${timestamp}] ${message}</span>\n`;
            logOutput.scrollTop = logOutput.scrollHeight;
            console.log(`[${type.toUpperCase()}] ${message}`);
        }

        function clearLogs() {
            document.getElementById('log-output').innerHTML = '';
        }

        // System status checks
        function checkSystemStatus() {
            log('=== System Status Check ===');
            
            // Browser support
            const browserSupport = {
                AudioContext: !!(window.AudioContext || window.webkitAudioContext),
                SpeechRecognition: !!(window.SpeechRecognition || window.webkitSpeechRecognition),
                MediaDevices: !!navigator.mediaDevices,
                getUserMedia: !!navigator.mediaDevices?.getUserMedia,
                Fetch: !!window.fetch
            };

            let supportHtml = '<h4>Browser Support:</h4>';
            for (const [feature, supported] in Object.entries(browserSupport)) {
                const status = supported ? 'pass' : 'fail';
                const icon = supported ? '‚úÖ' : '‚ùå';
                supportHtml += `<div class="test-result ${status}">${icon} ${feature}: ${supported ? 'Supported' : 'Not Supported'}</div>`;
                log(`${feature}: ${supported ? 'Supported' : 'Not Supported'}`, supported ? 'success' : 'error');
            }
            document.getElementById('browser-support').innerHTML = supportHtml;

            // API availability (simulated checks)
            const apiStatus = {
                'ElevenLabs API': 'Unknown - Requires actual API test',
                'Picovoice API': 'Unknown - Requires actual API test',
                'Backend Server': 'Unknown - Requires connection test'
            };

            let apiHtml = '<h4>API Status:</h4>';
            for (const [api, status] in Object.entries(apiStatus)) {
                apiHtml += `<div class="test-result warn">‚ö†Ô∏è ${api}: ${status}</div>`;
                log(`${api}: ${status}`, 'warn');
            }
            document.getElementById('api-availability').innerHTML = apiHtml;
        }

        // Test Web Audio API
        async function testAudioContext() {
            log('Testing Web Audio API...', 'info');
            const result = document.getElementById('audio-context-result');
            
            try {
                if (audioContext) {
                    audioContext.close();
                }
                
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                
                if (audioContext.state === 'suspended') {
                    await audioContext.resume();
                }

                // Test basic audio functionality
                const oscillator = audioContext.createOscillator();
                const gainNode = audioContext.createGain();
                
                oscillator.connect(gainNode);
                gainNode.connect(audioContext.destination);
                
                gainNode.gain.value = 0.1;
                oscillator.frequency.value = 440;
                oscillator.type = 'sine';
                
                oscillator.start();
                setTimeout(() => oscillator.stop(), 200);
                
                result.innerHTML = '<div class="test-result pass">‚úÖ Web Audio API: Working correctly</div>';
                log('Web Audio API test: PASSED', 'success');
                
                return true;
            } catch (error) {
                result.innerHTML = `<div class="test-result fail">‚ùå Web Audio API Error: ${error.message}</div>`;
                log(`Web Audio API test failed: ${error.message}`, 'error');
                return false;
            }
        }

        // Test Speech Recognition
        function testSpeechRecognition() {
            log('Testing Speech Recognition...', 'info');
            const result = document.getElementById('speech-recognition-result');
            
            if (!('webkitSpeechRecognition' in window) && !('SpeechRecognition' in window)) {
                result.innerHTML = '<div class="test-result fail">‚ùå Speech Recognition not supported in this browser</div>';
                log('Speech Recognition not supported', 'error');
                return;
            }

            try {
                const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
                speechRecognition = new SpeechRecognition();
                
                speechRecognition.continuous = true;
                speechRecognition.interimResults = true;
                speechRecognition.lang = 'en-US';

                speechRecognition.onstart = () => {
                    result.innerHTML = '<div class="test-result info">üé§ Speech Recognition started - please speak</div>';
                    log('Speech Recognition started', 'success');
                };

                speechRecognition.onresult = (event) => {
                    let interimTranscript = '';
                    let finalTranscript = '';

                    for (let i = event.resultIndex; i < event.results.length; i++) {
                        const transcript = event.results[i][0].transcript;
                        if (event.results[i].isFinal) {
                            finalTranscript += transcript;
                        } else {
                            interimTranscript += transcript;
                        }
                    }

                    let displayText = '';
                    if (finalTranscript) {
                        displayText += `<div class="test-result pass">‚úÖ Final: ${finalTranscript}</div>`;
                        log(`Speech Recognition final result: ${finalTranscript}`, 'success');
                    }
                    if (interimTranscript) {
                        displayText += `<div class="test-result info">üí≠ Interim: ${interimTranscript}</div>`;
                    }
                    
                    result.innerHTML = displayText;
                };

                speechRecognition.onerror = (event) => {
                    result.innerHTML = `<div class="test-result fail">‚ùå Speech Recognition Error: ${event.error}</div>`;
                    log(`Speech Recognition error: ${event.error}`, 'error');
                };

                speechRecognition.onend = () => {
                    log('Speech Recognition ended', 'info');
                };

                speechRecognition.start();
                
            } catch (error) {
                result.innerHTML = `<div class="test-result fail">‚ùå Speech Recognition setup failed: ${error.message}</div>`;
                log(`Speech Recognition setup failed: ${error.message}`, 'error');
            }
        }

        function stopSpeechRecognition() {
            if (speechRecognition) {
                speechRecognition.stop();
                log('Speech Recognition stopped', 'info');
            }
        }

        // Test TTS Simulation (using Web Audio API to play generated tone)
        async function testTTSSimulation() {
            log('Testing TTS audio playback simulation...', 'info');
            const result = document.getElementById('tts-result');
            
            try {
                if (!audioContext) {
                    audioContext = new (window.AudioContext || window.webkitAudioContext)();
                }

                if (audioContext.state === 'suspended') {
                    await audioContext.resume();
                }

                // Simulate TTS audio by creating a more complex audio pattern
                const duration = 2; // 2 seconds
                const sampleRate = audioContext.sampleRate;
                const buffer = audioContext.createBuffer(1, duration * sampleRate, sampleRate);
                const channelData = buffer.getChannelData(0);

                // Generate a speech-like pattern
                for (let i = 0; i < channelData.length; i++) {
                    const time = i / sampleRate;
                    // Create a modulated tone that sounds more like speech
                    const frequency = 200 + 100 * Math.sin(time * 3); // Varying frequency
                    const envelope = Math.exp(-time * 0.5) * (1 - Math.exp(-time * 10)); // Envelope
                    channelData[i] = envelope * Math.sin(frequency * 2 * Math.PI * time) * 0.3;
                }

                currentAudioSource = audioContext.createBufferSource();
                currentAudioSource.buffer = buffer;
                currentAudioSource.connect(audioContext.destination);
                
                currentAudioSource.onended = () => {
                    result.innerHTML += '<div class="test-result pass">‚úÖ TTS simulation completed</div>';
                    log('TTS simulation playback completed', 'success');
                };

                currentAudioSource.start();
                result.innerHTML = '<div class="test-result info">üîä Playing TTS simulation...</div>';
                log('TTS simulation started', 'info');
                
            } catch (error) {
                result.innerHTML = `<div class="test-result fail">‚ùå TTS simulation failed: ${error.message}</div>`;
                log(`TTS simulation failed: ${error.message}`, 'error');
            }
        }

        function stopTTSPlayback() {
            if (currentAudioSource) {
                currentAudioSource.stop();
                currentAudioSource = null;
                log('TTS playback stopped', 'info');
            }
        }

        // Test ElevenLabs API (connection test only)
        async function testElevenLabsAPI() {
            log('Testing ElevenLabs API connection...', 'info');
            const result = document.getElementById('elevenlabs-result');
            
            // In a real Electron app, this would go through the main process
            // Here we're just testing the API connectivity concept
            try {
                result.innerHTML = '<div class="test-result info">üîÑ Testing API connection...</div>';
                
                // Simulate API test (in real implementation, this would be done via IPC to main process)
                await new Promise(resolve => setTimeout(resolve, 1000));
                
                result.innerHTML = '<div class="test-result warn">‚ö†Ô∏è ElevenLabs API test requires main process - would work in full Electron app</div>';
                log('ElevenLabs API test simulated - requires IPC in real app', 'warn');
                
            } catch (error) {
                result.innerHTML = `<div class="test-result fail">‚ùå ElevenLabs API test failed: ${error.message}</div>`;
                log(`ElevenLabs API test failed: ${error.message}`, 'error');
            }
        }

        // Test Picovoice API
        async function testPicovoiceAPI() {
            log('Testing Picovoice wake word detection...', 'info');
            const result = document.getElementById('wake-word-result');
            
            try {
                result.innerHTML = '<div class="test-result info">üîÑ Testing Picovoice integration...</div>';
                
                // Check if the package is available
                const picovoiceAvailable = typeof window !== 'undefined' && window.location.protocol === 'file:';
                
                if (picovoiceAvailable) {
                    result.innerHTML = '<div class="test-result warn">‚ö†Ô∏è Picovoice would initialize in full Electron environment</div>';
                    log('Picovoice test simulated - requires full Electron app with assets', 'warn');
                } else {
                    result.innerHTML = '<div class="test-result info">‚ÑπÔ∏è Picovoice requires Electron environment and wake word model files</div>';
                    log('Picovoice requires Electron environment', 'info');
                }
                
            } catch (error) {
                result.innerHTML = `<div class="test-result fail">‚ùå Picovoice test failed: ${error.message}</div>`;
                log(`Picovoice test failed: ${error.message}`, 'error');
            }
        }

        // Test complete pipeline
        async function testFullPipeline() {
            log('=== Full Voice Pipeline Test ===', 'info');
            const result = document.getElementById('full-pipeline-result');
            let passedTests = 0;
            const totalTests = 4;
            
            result.innerHTML = '<div class="test-result info">üîÑ Running complete voice pipeline test...</div>';
            
            // Test 1: Audio Context
            if (await testAudioContext()) {
                passedTests++;
                log('Pipeline test 1/4: Audio Context - PASSED', 'success');
            } else {
                log('Pipeline test 1/4: Audio Context - FAILED', 'error');
            }
            
            // Test 2: Microphone access
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                stream.getTracks().forEach(track => track.stop());
                passedTests++;
                log('Pipeline test 2/4: Microphone access - PASSED', 'success');
            } catch (error) {
                log(`Pipeline test 2/4: Microphone access - FAILED (${error.message})`, 'error');
            }
            
            // Test 3: Speech Recognition availability
            if (window.SpeechRecognition || window.webkitSpeechRecognition) {
                passedTests++;
                log('Pipeline test 3/4: Speech Recognition availability - PASSED', 'success');
            } else {
                log('Pipeline test 3/4: Speech Recognition availability - FAILED', 'error');
            }
            
            // Test 4: API simulation
            await new Promise(resolve => setTimeout(resolve, 500));
            passedTests++;
            log('Pipeline test 4/4: API integration simulation - PASSED', 'success');
            
            // Results
            const successRate = (passedTests / totalTests) * 100;
            let resultClass = 'fail';
            let resultIcon = '‚ùå';
            
            if (successRate >= 75) {
                resultClass = 'pass';
                resultIcon = '‚úÖ';
            } else if (successRate >= 50) {
                resultClass = 'warn';
                resultIcon = '‚ö†Ô∏è';
            }
            
            result.innerHTML = `
                <div class="test-result ${resultClass}">
                    ${resultIcon} Full Pipeline Test: ${passedTests}/${totalTests} tests passed (${successRate}%)
                </div>
            `;
            
            log(`=== Pipeline Test Complete: ${passedTests}/${totalTests} passed ===`, successRate >= 75 ? 'success' : 'warn');
        }

        // Audio visualizer
        async function startAudioVisualizer() {
            log('Starting audio visualizer...', 'info');
            const canvas = document.getElementById('audioCanvas');
            const ctx = canvas.getContext('2d');
            
            try {
                mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true });
                
                if (!audioContext) {
                    audioContext = new (window.AudioContext || window.webkitAudioContext)();
                }
                
                analyser = audioContext.createAnalyser();
                analyser.fftSize = 256;
                
                const source = audioContext.createMediaStreamSource(mediaStream);
                source.connect(analyser);
                
                const bufferLength = analyser.frequencyBinCount;
                const dataArray = new Uint8Array(bufferLength);
                
                function draw() {
                    animationFrame = requestAnimationFrame(draw);
                    
                    analyser.getByteFrequencyData(dataArray);
                    
                    ctx.fillStyle = 'rgba(0, 0, 0, 0.1)';
                    ctx.fillRect(0, 0, canvas.width, canvas.height);
                    
                    const barWidth = (canvas.width / bufferLength) * 2.5;
                    let x = 0;
                    
                    for (let i = 0; i < bufferLength; i++) {
                        const barHeight = (dataArray[i] / 255) * canvas.height;
                        
                        const r = barHeight + 25 * (i / bufferLength);
                        const g = 250 * (i / bufferLength);
                        const b = 50;
                        
                        ctx.fillStyle = `rgb(${r},${g},${b})`;
                        ctx.fillRect(x, canvas.height - barHeight, barWidth, barHeight);
                        
                        x += barWidth + 1;
                    }
                }
                
                draw();
                log('Audio visualizer started successfully', 'success');
                
            } catch (error) {
                log(`Failed to start audio visualizer: ${error.message}`, 'error');
            }
        }

        function stopAudioVisualizer() {
            if (animationFrame) {
                cancelAnimationFrame(animationFrame);
                animationFrame = null;
            }
            
            if (mediaStream) {
                mediaStream.getTracks().forEach(track => track.stop());
                mediaStream = null;
            }
            
            const canvas = document.getElementById('audioCanvas');
            const ctx = canvas.getContext('2d');
            ctx.fillStyle = 'rgba(0, 0, 0, 1)';
            ctx.fillRect(0, 0, canvas.width, canvas.height);
            
            log('Audio visualizer stopped', 'info');
        }

        // Initialize on load
        window.onload = () => {
            log('Luna Agent Voice Pipeline Test Suite initialized', 'success');
            checkSystemStatus();
        };

        // Cleanup on unload
        window.onbeforeunload = () => {
            stopAudioVisualizer();
            stopSpeechRecognition();
            stopTTSPlayback();
            if (audioContext) {
                audioContext.close();
            }
        };
    </script>
</body>
</html>